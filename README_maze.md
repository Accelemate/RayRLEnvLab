# 迷宫强化学习训练

使用 Gymnasium 和 Ray RLlib 训练智能体完成迷宫导航任务。

## 功能特点

1. **自定义迷宫环境**：实现了简单的网格迷宫环境，智能体需要从起点到达目标点
2. **PPO算法训练**：使用 Ray RLlib 的 PPO 算法进行训练
3. **使用RLlib默认模型**：采用官方推荐的默认模型配置（通过`fcnet_hiddens`参数），无需自定义`TorchModelV2`，代码更简洁、更易维护
4. **训练曲线记录**：自动记录并绘制训练过程中的奖励变化曲线
5. **模型保存与测试**：训练完成后保存模型并测试性能

## 技术亮点

- **使用成熟方案**：采用RLlib官方推荐的默认模型配置方式，而非自定义神经网络模型
- **代码简洁**：减少了大量自定义模型代码，提高了可读性和可维护性
- **易于调整**：通过简单的配置参数即可调整网络结构（隐藏层数量、大小等）

## 环境说明

- **迷宫大小**：5x5 网格
- **动作空间**：4个离散动作（上、下、左、右）
- **观测空间**：智能体的当前位置坐标 (x, y)
- **奖励设计**：
  - 到达目标：+10.0
  - 撞墙：-0.5
  - 每步：-0.01（鼓励快速到达）
  - 距离奖励：根据距离目标的远近给予额外奖励

## 运行方式

```bash
python maze_rl_training.py
```

## 输出结果

训练完成后会生成以下文件：

- `trained_maze_model/`：训练好的模型检查点
- `results/training_curve.png`：训练奖励曲线图
- `results/episode_rewards.npy`：每轮奖励数据
- `results/episode_lengths.npy`：每轮步数数据

## 依赖项

确保已安装以下依赖：

- gymnasium
- ray[rllib]
- torch
- numpy
- matplotlib

## 训练参数

- 训练轮次：100 轮
- 并行环境数：4
- 网络结构：三层全连接网络 [128, 128, 64]
- 学习率：3e-4
- 批次大小：4000
- 折扣因子：0.99

可以在代码中调整这些参数以优化训练效果。特别是可以通过修改`model["fcnet_hiddens"]`参数来调整网络结构。

